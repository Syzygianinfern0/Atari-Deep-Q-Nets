{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "import cv2\n",
    "warnings.filterwarnings('ignore')\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "# Test the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "env.reset()\n",
    "for _ in range(3000):\n",
    "    env.step(env.action_space.sample())\n",
    "    env.render('human')\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Getting to Know the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(210, 160, 3)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space.n)\n",
    "possible_actions = np.array(np.identity(env.action_space.n,dtype=int).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocessing\n",
    "To remove unnecessary parts of the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Greyscale frame \n",
    "    gray = rgb2gray(frame)\n",
    "    \n",
    "    # Crop the screen (remove the part below the player)\n",
    "    # [Up: Down, Left: right]\n",
    "    cropped_frame = gray[8:-12,4:-12]\n",
    "    \n",
    "    # Normalize Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "    \n",
    "    # Resize\n",
    "    # Thanks to Mikołaj Walkowiak\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [110,84])\n",
    "    \n",
    "    return preprocessed_frame # 110x84x1 frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stack_size = 4\n",
    "\n",
    "stacked_frames  =  deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((110, 84), dtype=np.int) for _ in range(stack_size)], maxlen=stack_size)\n",
    "        for _ in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    \n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return  stacked_state, stacked_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [110, 84, 4]      # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) \n",
    "action_size = env.action_space.n # 8 possible actions\n",
    "learning_rate =  0.00025      # Alpha (aka learning rate)\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 50            # Total episodes for training\n",
    "max_steps = 50000              # Max possible steps in an episode\n",
    "batch_size = 64                # Batch size\n",
    "\n",
    "# Exploration parameters for epsilon greedy strategy\n",
    "explore_start = 1.0            # exploration probability at start\n",
    "explore_stop = 0.01            # minimum exploration probability \n",
    "decay_rate = 0.00001           # exponential decay rate for exploration prob\n",
    "\n",
    "# Q learning hyperparameters\n",
    "gamma = 0.9                    # Discounting rate\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\n",
    "\n",
    "### PREPROCESSING HYPERPARAMETERS\n",
    "stack_size = 4                 # Number of frames stacked\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.make_model()\n",
    "        \n",
    "    def make_model(self):\n",
    "        self.model = tf.keras.Sequential([\n",
    "            # ------------Layer 1------------------\n",
    "            tf.keras.layers.Conv2D(32, (3, 3),\n",
    "                                   activation=tf.keras.activations.elu,\n",
    "                                   input_shape=(110, 84, 4)),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            # ------------Layer 2------------------\n",
    "            tf.keras.layers.Conv2D(64, (3, 3),\n",
    "                                   activation=tf.keras.activations.elu),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            # ------------Layer 3------------------\n",
    "            \n",
    "            tf.keras.layers.Conv2D(128, (3, 3),\n",
    "                                   activation=tf.keras.activations.elu\n",
    "                                   ),\n",
    "            tf.keras.layers.MaxPooling2D(),\n",
    "            # ------------Layer 4------------------            \n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(512, activation=tf.keras.activations.relu),\n",
    "            # ------------Layer 5------------------\n",
    "            tf.keras.layers.Dense(128, activation=tf.keras.activations.relu),\n",
    "            # ------------Layer 6------------------\n",
    "            tf.keras.layers.Dense(8, activation=tf.keras.activations.softmax)   \n",
    "        ])\n",
    "        self.model.summary()\n",
    "        self.model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate),\n",
    "                           loss=tf.keras.losses.mean_squared_error,\n",
    "#                            metrics=[tf.keras.metrics.mean_absolute_error]\n",
    "                          ) \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\Space Invaders\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 108, 82, 32)       1184      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 54, 41, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 52, 39, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 26, 19, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 8, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               6291968   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 1032      \n",
      "=================================================================\n",
      "Total params: 6,452,200\n",
      "Trainable params: 6,452,200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate memory\n",
    "env.reset()\n",
    "memory = Memory(max_size = memory_size)\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        state = env.reset()\n",
    "        \n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    # Get the next_state, the rewards, done by taking a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    env.render()\n",
    "    \n",
    "    # Stack the frames\n",
    "    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "    \n",
    "    \n",
    "    # If the episode is finished (we're dead 3x)\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "        \n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Start a new episode\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Our new state is now the next_state\n",
    "        state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function will do the part\n",
    "With ϵϵ select a random action atat, otherwise select at=argmaxaQ(st,a)\n",
    "\"\"\"\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    ## EPSILON GREEDY STRATEGY\n",
    "    # Choose action a from state s using epsilon greedy.\n",
    "    ## First we randomize a number\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "    \n",
    "    if (explore_probability > exp_exp_tradeoff):\n",
    "        # Make a random action (exploration)\n",
    "#         choice = random.randint(1,len(possible_actions))-1\n",
    "#         action = possible_actions[choice]\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        # Estimate the Qs values state\n",
    "#         Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        Qs = DQNetwork.model.predict(state)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "                \n",
    "                \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11255705, 0.11255709, 0.11255342, 0.11255745, 0.1125561 ,\n",
       "       0.11255626, 0.11255631, 0.11255709, 0.11255709, 0.11255709,\n",
       "       0.11255745, 0.11255745, 0.11255504, 0.1125561 , 0.1125561 ,\n",
       "       0.11255631, 0.11255745, 0.11255367, 0.11255677, 0.11254545,\n",
       "       0.11255745, 0.11255745, 0.11255702, 0.11255437, 0.11255745,\n",
       "       0.11255626, 0.11255631, 0.11255745, 0.11255626, 0.11255745,\n",
       "       0.11255364, 0.11255745, 0.11255312, 0.11255696, 0.11255626,\n",
       "       0.11255745, 0.1125561 , 0.11255454, 0.11255745, 0.11255631,\n",
       "       0.11255709, 0.11255709, 0.11255677, 0.11255747, 0.11255745,\n",
       "       0.11255044, 0.11255709, 0.1125561 , 0.11255048, 0.11255677,\n",
       "       0.11255626, 0.11255366, 0.11255626, 0.1125561 , 0.11254623,\n",
       "       5.11255517, 0.11254817, 0.11255745, 0.11255381, 0.11255146,\n",
       "       0.11255677, 0.11255631, 0.11255745, 0.11255747])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saver will help us to save our model\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    decay_step = 0\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        # Set step to 0\n",
    "        step = 0\n",
    "\n",
    "        # Initialize the rewards of the episode\n",
    "        episode_rewards = []\n",
    "\n",
    "        # Make a new episode and observe the first state\n",
    "        state = env.reset()\n",
    "\n",
    "        # Remember that stack frame function also call our preprocess function.\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "\n",
    "        while step < max_steps:\n",
    "            step += 1\n",
    "\n",
    "            #Increase decay_step\n",
    "            decay_step +=1\n",
    "\n",
    "            # Predict the action to take and take it\n",
    "            action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\n",
    "\n",
    "            #Perform the action and get the next_state, reward, and done information\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if episode_render:\n",
    "                env.render()\n",
    "\n",
    "            # Add the reward to total reward\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "            # If the game is finished\n",
    "            if done:\n",
    "                # The episode ends so no next state\n",
    "                next_state = np.zeros((110,84), dtype=np.int)\n",
    "\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                # Set step = max_steps to end the episode\n",
    "                step = max_steps\n",
    "\n",
    "                # Get the total reward of the episode\n",
    "                total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                print('Episode: {}'.format(episode),\n",
    "                              'Total reward: {}'.format(total_reward),\n",
    "                              'Explore P: {:.4f}'.format(explore_probability),\n",
    "                            'Training Loss {:.4f}'.format(loss))\n",
    "\n",
    "                rewards_list.append((episode, total_reward))\n",
    "\n",
    "                # Store transition <st,at,rt+1,st+1> in memory D\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "            else:\n",
    "                # Stack the frame of the next_state\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                # st+1 is now our current state\n",
    "                state = next_state\n",
    "\n",
    "\n",
    "            ### LEARNING PART            \n",
    "            # Obtain random mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "            actions_mb = np.array([each[1] for each in batch])\n",
    "            rewards_mb = np.array([each[2] for each in batch]) \n",
    "            next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "            dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "            target_Qs_batch = []\n",
    "\n",
    "            # Get Q values for next_state \n",
    "#             Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "            Qs_next_state = DQNetwork.model.predict(next_states_mb)\n",
    "\n",
    "            # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\n",
    "            for i in range(0, len(batch)):\n",
    "                terminal = dones_mb[i]\n",
    "\n",
    "                # If we are in a terminal state, only equals reward\n",
    "                if terminal:\n",
    "                    target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "                else:\n",
    "                    target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                    target_Qs_batch.append(target)\n",
    "\n",
    "\n",
    "            targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "#             loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "#                                     feed_dict={DQNetwork.inputs_: states_mb,\n",
    "#                                                DQNetwork.target_Q: targets_mb,\n",
    "#                                                DQNetwork.actions_: actions_mb})\n",
    "\n",
    "            break\n",
    "            \n",
    "            [DQNetwork.model.fit(states_mb, targets_mb[i], epochs=5) for i in range(len(target_Qs_batch))]\n",
    "\n",
    "            \n",
    "        break\n",
    "        \n",
    "        # Save model every 5 episodes\n",
    "        if episode % 5 == 0:\n",
    "            save_path = DQNetwork.model.save('model.h5')\n",
    "            print(\"Model Saved\")\n",
    "            \n",
    "        \n",
    "targets_mb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
